One of the fundamental risks of automated content suggestions for news articles is the potential for large language models to hallucinate and generate information from elsewhere in its training set, rather than the information that is provided in the immediate query that is provided; in particular, it is possible for the LLM to also reference prior information provided in the few-shot prompt as information to continue to reference and spit that out as well.

There are a couple of ways to mitigate the risks of automated content generation. The most basic are to tone down the temperature of the API call (here I have set it to 0.2), which makes the LLM response more conservative and can mitigate harmful or inappropriate outputs, as well as include more few-shot prompts. On the advanced side, another approach could be to use an extraction framework like Pydantic’s tagging chain or Langchain to extract key facts from an article, upon which to then “ground” the production of social media copy and headlines. Such approaches can provide more transparency in the process while effectively utilizing LLM’s creativity and polish to ensure the final product is still comprehensible and pleasing.

Since LLMs are trained on a huge corpus of Internet data, there are potential issues with the data they are trained on and referencing in content generation being 1) inappropriate for a news context, 2) irrelevant to the specific context at hand (but the LLM thinks it is relevant), or 3) outdated and thus causing more accuracy issues. One approach I would be interested in trying would be to  specifically customize the few-shot prompts with regard to articles in a similar vein as the one we want to generate headlines and social copy for. In particular, I noticed that the sports article had a different tone than the rest of the articles, and this could influence the headline and social copy style, especially if they are for different social accounts or just various verticals that want to brand themselves differently. I would be interested in trying an approach that takes as input both the text and a list of keywords or tags, and then generates past few-shot examples based on that for greater accuracy!
